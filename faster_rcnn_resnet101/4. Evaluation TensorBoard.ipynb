{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa082d3-1ce9-4020-a41d-9c72aad71756",
   "metadata": {},
   "source": [
    "# 7144COMP/CW2: Bird Multiple Object Detection Using Faster R-CNN ResNet101 Network \n",
    "## PART IV: Model evaluation and deployment\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this notebook, I will evaluate my model through TensorBoard while using the generated metrics to determine model convergence (both validation loss and Intersection over Union (IoU) at both 0.5 and 0.75 are considered). \n",
    "\n",
    "The number of epochs to train the model is set to 1, the reason for this choice was explained in the training notebook. In addition, during the 1st epoch of training, the model converged around the final loss value (smoothed loss value with a weight of 0.8).\n",
    "\n",
    "\n",
    "\n",
    "For the current task, the following steps have been undertaken: \n",
    "\n",
    "- Launch TensorBoard displaying both the train and evaluation metrics for the given session. \n",
    "- Provide justification for the number of epochs used for training your object detection model\n",
    "\n",
    "### Next\n",
    "\n",
    "In the next notebook which is an extension to the present, I will:\n",
    "\n",
    "- Freeze my trained model in correct format for model inferencing\n",
    "- Develop a Jupyter Notebook to perform inference on the frozen model using unseen test images\n",
    "- Discuss my results.\n",
    "\n",
    "### Prerequisites\n",
    "This notebook runs locally on the environment *tf-gpu*.\n",
    "- Environment Setup (see Part 0)\n",
    "- Preprocessing (see Part 1)\n",
    "- Training (see Part 2)\n",
    "- Run the necessary evaluation scripts (see Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a13b846-f597-4ef3-a11c-a87913e96351",
   "metadata": {},
   "source": [
    "## 1. Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3cd42c-70f0-4302-8fc2-5d11fed5b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37d3796a-cc91-4ebe-913b-df561cd6d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current directory\n",
    "current_dir = os.getcwd()\n",
    "# Model training directory and config pipeline\n",
    "model_dir = os.path.join(current_dir, 'training')\n",
    "pipeline_config_path = 'fasterrcnn_config.config'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b3465-4546-404c-9190-0f751ba09806",
   "metadata": {},
   "source": [
    "## 2. TensorBoard \n",
    "### 2.1. Monitor region proposal losses in real-time\n",
    "Here ```logdir``` points to the train directory, by launching the next cell, different loss graphs for region proposal network will be imported by TensorBoard and updated each step.\n",
    "\n",
    "The losses for the Region Proposal Network:\n",
    "\n",
    "- ```Loss/RPNLoss/localization_loss```: Localization Loss or the Loss of the Bounding Box regressor for the RPN\n",
    "\n",
    "- ```Loss/RPNLoss/objectness_loss```: Loss of the Classifier that classifies if a bounding box is an object of interest or background\n",
    "\n",
    "The losses for the Final Classifier:\n",
    "\n",
    "- ```Loss/BoxClassifierLoss/classification_loss```: Loss for the classification of detected objects into various classes: Cat, Dog, Airplane etc\n",
    "\n",
    "- ```BoxClassifierLoss/localization_loss```: Localization Loss or the Loss of the Bounding Box regressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbae6254-c51b-4c1d-89e8-2bfd56be540c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-23 00:52:43.287233: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-23 00:52:43.934477: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2022-12-23 00:52:43.934530: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2022-12-23 00:52:43.934540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-23 00:52:44.720641: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2022-12-23 00:52:44.720671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: CMPR01\n",
      "2022-12-23 00:52:44.720679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: CMPR01\n",
      "2022-12-23 00:52:44.720752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.161.3\n",
      "2022-12-23 00:52:44.720778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
      "2022-12-23 00:52:44.720786: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 460.91.3 does not match DSO version 470.161.3 -- cannot find working devices in this configuration\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.11.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir $current_dir'/training/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9a4d1-4f8f-41ce-953f-22f7880fdbfc",
   "metadata": {},
   "source": [
    "### 2.2. Display the train and evaluation metrics for the given session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef255d4-e4a0-441a-850c-a28aa7da3b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-24 18:02:47.277700: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-24 18:02:47.924591: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2022-12-24 18:02:47.924640: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2022-12-24 18:02:47.924648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-24 18:02:48.728956: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2022-12-24 18:02:48.728986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: CMPR01\n",
      "2022-12-24 18:02:48.728993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: CMPR01\n",
      "2022-12-24 18:02:48.729057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.161.3\n",
      "2022-12-24 18:02:48.729078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
      "2022-12-24 18:02:48.729085: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 460.91.3 does not match DSO version 470.161.3 -- cannot find working devices in this configuration\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.11.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# This requires stopping the previous TensorBoard Server\n",
    "# Execute tensorboard and point logdir to the eval folder to load\n",
    "# DetectionBoxes Precision and Recall Metrics at STEP 28000\n",
    "!tensorboard --logdir $current_dir'/training/eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bdc29b-be80-4080-8bf1-9abdabf2345c",
   "metadata": {},
   "source": [
    "## 3. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73404608-f3d5-4843-bf84-224741185833",
   "metadata": {},
   "source": [
    "\n",
    "- Our mean average Precision scores at 0.5 and 0.75 levels of IoU were ```mAP@.5 = 0.8711``` and ```mAP@.75 = 0.6416``` respectively, which are above mAP scores of the original Faster R-CNN ResNet101 V1 640x640 (trained on MS COCO Dataset). Our model improved its precision for object detection on our custom database thanks to transfer learning.\n",
    "- Our average Recall (sensitivity) score was ```AR = 0.6083```.\n",
    "\n",
    "Idealy the better mAP and AR the better our model's performance.\n",
    "\n",
    "### Justification for the number of epochs used for training your object detection model\n",
    "During the training, one step took on average 1.95 seconds, an epoch consists of 28000 steps (batch_size=1), so the total duration of an epoch would be approx 15 hours. \n",
    "\n",
    "It was not possible to increase the batch size due to memory limitations. \n",
    "\n",
    "The experiment conducted on the Cloud showed that with 40 epochs the model demonstrated the same level of accuracy and noise compared to 1 training epoch on the local machine. Training the model with higher batch_size led to faster results (43 minutes).\n",
    "\n",
    "However, this is not quite sufficient to obtain an optimal level of inference precision and the lowest noise possible.\n",
    "\n",
    "#### **Comparison with Roboflow Cloud-based experiment (using the same model and dataset)**\n",
    "The same model with almost the same pipeline was trained on Roboflow Cloud (detailed metrics can be found at the end of this notebook): \n",
    "\n",
    "- **Without data augmentation**: 300 epochs were needed to converge, the model achieved 95.6% mAP 90.5% precision and 91.2% recall with an average class precision of 95% on the validation dataset.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/roboflow-platform-cache/RjBpFWbVLQdI2NaOrqg24Eooatr2/qYHiTyjFVuJ6MWIK56Sh/2/results.png\" width=\"800\" />\n",
    "\n",
    "- **With data augmentation**: using almost the same augmentation steps and the same hyperparameters (except num_epochs) : 40 epochs were needed to converge, the model achieved **90.1% mAP 88.4% precision and 82.8% recall** with an **average class precision of 90%** on the validation dataset.\n",
    "\n",
    "- We can conclude that data augmentation was necessary to reduce the number of epochs and mitigate the risk of over-fitting.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/roboflow-platform-cache/RjBpFWbVLQdI2NaOrqg24Eooatr2/qYHiTyjFVuJ6MWIK56Sh/4/results.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ebbd6e-3da3-4881-83dc-06f2cb9ddc84",
   "metadata": {},
   "source": [
    "### Next\n",
    "\n",
    "- Freeze the trained model in correct format for model inferencing\n",
    "- Develop a Jupyter Notebook to perform inference on the frozen model using unseen test images\n",
    "- Discuss my results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlowGPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
