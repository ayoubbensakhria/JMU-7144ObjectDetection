{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa082d3-1ce9-4020-a41d-9c72aad71756",
   "metadata": {},
   "source": [
    "# 7144COMP/CW2: Bird Multiple Object Detection Using Faster R-CNN and SSD\n",
    "## PART IV: Model evaluation and deployment\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this notebook, I will evaluate my model through TensorBoard while using the generated metrics to determine model convergence (both validation loss and Intersection over Union (IoU) at both 0.5 and 0.75 are considered). \n",
    "\n",
    "The number of epochs to train the model is set to 1, the reason for this choice was explained in the training notebook. In addition, during the 1st epoch of training, the model converged around the final loss value (smoothed loss value with a weight of 0.8).\n",
    "\n",
    "\n",
    "\n",
    "For the current task, the following steps have been undertaken: \n",
    "\n",
    "- Launch TensorBoard displaying both the train and evaluation metrics for the given session. \n",
    "- Provide justification for the number of epochs used for training your object detection model\n",
    "\n",
    "### Next\n",
    "\n",
    "In the next notebook which is an extension to the present, I will:\n",
    "\n",
    "- Freeze my trained model in correct format for model inferencing\n",
    "- Develop a Jupyter Notebook to perform inference on the frozen model using unseen test images\n",
    "- Discuss my results.\n",
    "\n",
    "### Prerequisites\n",
    "This notebook runs locally on the environment *tf-gpu*.\n",
    "- Environment Setup (see Part 0)\n",
    "- Preprocessing (see Part 1)\n",
    "- Training (see Part 2)\n",
    "- Run the necessary evaluation scripts (see Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a13b846-f597-4ef3-a11c-a87913e96351",
   "metadata": {},
   "source": [
    "## 1. Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3cd42c-70f0-4302-8fc2-5d11fed5b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d3796a-cc91-4ebe-913b-df561cd6d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Faster R-CNN\n",
    "# Model training directory and config pipeline\n",
    "model_dir = os.path.join(current_dir, 'training')\n",
    "pipeline_config_path = 'fasterrcnn_config.config'\n",
    "\n",
    "# SSD\n",
    "# Model training directory and config pipeline\n",
    "model_dir_ssd = os.path.join(current_dir, 'training_ssd')\n",
    "pipeline_config_path_ssd = 'ssd_config.config'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b3465-4546-404c-9190-0f751ba09806",
   "metadata": {},
   "source": [
    "## 2. TensorBoard \n",
    "### 2.1. Monitor region proposal losses, evaluation metrics\n",
    "Here ```logdir``` points to the training directory, by launching the next cell, different loss graphs for region proposal network will be imported by TensorBoard from ```training/train``` folder, whereas evaluation metrics for the given session will be imported from the ```training/eval``` folder.\n",
    "\n",
    "The losses for the Region Proposal Network:\n",
    "\n",
    "- ```Loss/RPNLoss/localization_loss```: Localization Loss or the Loss of the Bounding Box regressor for the RPN\n",
    "\n",
    "- ```Loss/RPNLoss/objectness_loss```: Loss of the Classifier that classifies if a bounding box is an object of interest or background\n",
    "\n",
    "The losses for the Final Classifier:\n",
    "\n",
    "- ```Loss/BoxClassifierLoss/classification_loss```: Loss for the classification of detected objects into various classes: Cat, Dog, Airplane etc\n",
    "\n",
    "- ```BoxClassifierLoss/localization_loss```: Localization Loss or the Loss of the Bounding Box regressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32dd5d-575f-4364-8bd1-79fe5ac5bab9",
   "metadata": {},
   "source": [
    "### Display the train and evaluation metrics for the given session \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf742d-071b-4d3f-950a-f87eef6b3298",
   "metadata": {},
   "source": [
    "#### Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae6254-c51b-4c1d-89e8-2bfd56be540c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-09 06:39:31.064229: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-09 06:39:32.780153: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-09 06:39:32.780253: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-09 06:39:32.780275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-09 06:39:34.912356: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-01-09 06:39:34.912397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: CMPR01\n",
      "2023-01-09 06:39:34.912409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: CMPR01\n",
      "2023-01-09 06:39:34.912504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.161.3\n",
      "2023-01-09 06:39:34.912536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
      "2023-01-09 06:39:34.912545: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 460.91.3 does not match DSO version 470.161.3 -- cannot find working devices in this configuration\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.11.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir $current_dir'/training/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6e17c-3633-487f-aa73-7bedc1dafa7d",
   "metadata": {},
   "source": [
    "#### SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baaee2f-fb0f-43d1-b59f-e368544886aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-11 23:27:46.112863: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-11 23:27:46.771068: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-11 23:27:46.771116: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-11 23:27:46.771124: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-11 23:27:47.556493: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-01-11 23:27:47.556518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: CMPR01\n",
      "2023-01-11 23:27:47.556525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: CMPR01\n",
      "2023-01-11 23:27:47.556573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.161.3\n",
      "2023-01-11 23:27:47.556592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
      "2023-01-11 23:27:47.556599: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 460.91.3 does not match DSO version 470.161.3 -- cannot find working devices in this configuration\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.11.0 at http://localhost:6007/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir $current_dir'/training_ssd/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bdc29b-be80-4080-8bf1-9abdabf2345c",
   "metadata": {},
   "source": [
    "## 3. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73404608-f3d5-4843-bf84-224741185833",
   "metadata": {},
   "source": [
    "### Training vs Validation losses\n",
    "\n",
    "- The training total loss was ```train_total_loss = 0.09```, whereas the validation total loss was ```val_total_loss = 0.3``` at the last step of training. \n",
    "- Both losses seem to converge down towards lower error levels, which is a good sign that the model is learning.\n",
    "\n",
    "- The validation loss was higher than the training loss because the model was evaluated on data it has not seen during training\n",
    "\n",
    "<img src=\"https://ayoubb.com/wp-content/uploads/2023/01/Flowchart-4.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ae06a-2007-4b3c-8c11-a48778846d76",
   "metadata": {},
   "source": [
    "### Precision Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c0743-228d-49fe-8c40-0e9b8dd1b067",
   "metadata": {},
   "source": [
    "#### Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4ab9a-2d0e-4759-9c16-f57f50965d9a",
   "metadata": {},
   "source": [
    "<table>\n",
    "\t<thead>\n",
    "\t\t<tr>\n",
    "\t\t\t<th>Metric</th>\n",
    "\t\t\t<th>Value</th>\n",
    "\t\t</tr>\n",
    "\t</thead>\n",
    "\t<tbody>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) @ [IoU=0.50:0.95, area=all]</td>\n",
    "\t\t\t<td>0.559</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) @ [IoU=0.50, area=all]</td>\n",
    "\t\t\t<td>0.851</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) @ [IoU=0.75, area=all]</td>\n",
    "\t\t\t<td>0.657</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) @ [IoU=0.50:0.95, area=small]</td>\n",
    "\t\t\t<td>0.125</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) @ [IoU=0.50:0.95, area=medium]</td>\n",
    "\t\t\t<td>0.402</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) @ [IoU=0.50:0.95, area=large]</td>\n",
    "\t\t\t<td>0.643</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) @ [IoU=0.50:0.95, area=all, maxDets=1]</td>\n",
    "\t\t\t<td>0.624</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) @ [IoU=0.50:0.95, area=all, maxDets=10]</td>\n",
    "\t\t\t<td>0.665</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) @ [IoU=0.50:0.95, area=all, maxDets=100]</td>\n",
    "\t\t\t<td>0.670</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) @ [IoU=0.50:0.95, area=small, maxDets=100]</td>\n",
    "\t\t\t<td>0.270</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) @ [IoU=0.50:0.95, area=medium, maxDets=100]</td>\n",
    "\t\t\t<td>0.561</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) @ [IoU=0.50:0.95, area=large, maxDets=100]</td>\n",
    "\t\t\t<td>0.732</td>\n",
    "\t\t</tr>\n",
    "\t</tbody>\n",
    "</table>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dbd66d-0b41-441f-a07c-928e0c7ed5d0",
   "metadata": {},
   "source": [
    "- The ```mAP``` at an ```IoU``` threshold of ```0.50``` is ```0.851```, which is relatively high. This suggests that the model is detecting a high fraction of objects correctly and has a low false positive rate.\n",
    "\n",
    "- The ```mAP``` at an ```IoU``` threshold of ```0.75``` is ```0.657```, which is lower than the mAP at an IoU threshold of 0.50. This may indicate that the model is more conservative (less prone to false positives) when the overlap threshold is stricter.\n",
    "\n",
    "- The average recall ```AR``` at an IoU threshold of ```0.50:0.95``` is ```0.670```, which is relatively high. This suggests that the model is detecting a high fraction of actual objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7719ed-827e-469a-8ec9-6f9b83b5e847",
   "metadata": {},
   "source": [
    "### Per-Class Precision Metrics (mAP@.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4208abcb-3d5a-4efe-a5a5-b1c1d382ee98",
   "metadata": {},
   "source": [
    "<table>\n",
    "\t<thead>\n",
    "\t\t<tr>\n",
    "\t\t\t<th>Class</th>\n",
    "\t\t\t<th>AP@0.5IOU</th>\n",
    "\t\t</tr>\n",
    "\t</thead>\n",
    "\t<tbody>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Erithacus Rubecula</td>\n",
    "\t\t\t<td>0.001856</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Periparus ater</td>\n",
    "\t\t\t<td>0.843017</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Pica pica</td>\n",
    "\t\t\t<td>0.885294</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Turdus merula</td>\n",
    "\t\t\t<td>0.829829</td>\n",
    "\t\t</tr>\n",
    "\t</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "- The classes ```Pica_pica``` and ```Periparus_ater``` have the highest average precision, followed by ```Turdus_merula```. ```Erithacus_Rubecula``` has the lowest average precision.</p>\n",
    "\n",
    "- It may be worth investigating why ```Erithacus_Rubecula``` has a lower average precision and consider ways to improve the model's performance for that class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb52419f-b4be-4db0-8403-25406e3d1335",
   "metadata": {},
   "source": [
    "#### SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9de04a3-5d4d-4497-bf0e-e54a09a31cce",
   "metadata": {},
   "source": [
    "<table>\n",
    "\t<thead>\n",
    "\t\t<tr>\n",
    "\t\t\t<th>Metric</th>\n",
    "\t\t\t<th>Value</th>\n",
    "\t\t</tr>\n",
    "\t</thead>\n",
    "\t<tbody>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) @[ IoU=0.50:0.95 ]</td>\n",
    "\t\t\t<td>0.001</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) @[ IoU=0.50 ]</td>\n",
    "\t\t\t<td>0.005</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) @[ IoU=0.75 ]</td>\n",
    "\t\t\t<td>0.000</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) @[ IoU=0.50:0.95 ]</td>\n",
    "\t\t\t<td>0.028</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) @[ IoU=0.50:0.95 ]</td>\n",
    "\t\t\t<td>0.047</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) @[ IoU=0.50:0.95 ]</td>\n",
    "\t\t\t<td>0.076</td>\n",
    "\t\t</tr>\n",
    "\t</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb83b7-b0ab-4e15-ba3b-c717693a3914",
   "metadata": {},
   "source": [
    "The values provided in the table are very low, specifically all the APs are in the range of 0.001 to 0.002 and the AR in the range of 0.028 to 0.076. This indicates that the model is not performing well and it has a poor detection rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba94b1-a8fa-425c-a233-85663adc3656",
   "metadata": {},
   "source": [
    "### Justification for the number of epochs used for training your object detection model\n",
    "\n",
    "```num_epochs``` : A number of epochs of ```7``` means that the model will make seven passes through the entire training dataset. While there is no formal definition of the term epoch, we used the following formula to calculate ```num_epochs```: \n",
    "\n",
    "$$\n",
    "\\text { num_epochs }=\\text { training_steps * batch_size } / \\text { training_set_size }\n",
    "$$\n",
    "\n",
    "\n",
    "Taking into consideration some limitations such as time, hardware (Memory, Number of GPUs available), the following approach was applied to find the optimal ```num_epochs```:\n",
    "\n",
    "- Incremental training: by gradually increasing the ```num_epochs``` from 1 to 10, the model automatically starts training from the last checkpoint, this way, we can continuously monitor the model's performance on the validation set (```validation_loss```) as it is training, and stop training when the performance on the validation set stops improving. This can help to prevent overfitting and ensure that the model is not trained for more epochs than necessary.\n",
    "\n",
    "- I found that at the step ```21000```, which is the 7th epoch, the validation loss and train loss curves stoped decreasing, which is a sign that our model stopped learning from the training dataset at that particular step.\n",
    "\n",
    "Increasing the number of epochs (num_epochs) during training can help improve the performance of an object detection model in a few different ways:\n",
    "\n",
    "- **Increased model convergence**: by allowing the model to see the training data more times, which can help it learn more effectively and converge on a better solution.\n",
    "\n",
    "- **Improved generalisation**: A model that has been trained for more epochs may be better able to generalize to new, unseen data. This is because the model has been exposed to more diverse examples during training and has had more opportunities to learn about the underlying patterns in the data.\n",
    "\n",
    "- **More time for optimisation**: Training for more epochs gives the model more time to adjust its weights and biases through the optimization process. This can lead to improved model performance, especially if the learning rate is set appropriately.\n",
    "\n",
    "Nevertheless, increasing the number of epochs also increases the training time and can lead to overfitting if the model is trained for too many epochs. Overfitting occurs when a model becomes *too specialized to the training data and performs poorly on new, unseen data*.\n",
    "\n",
    "Note: It was not possible to increase the batch size due to memory limitations. \n",
    "\n",
    "#### **Comparison with Roboflow Cloud-based experiment (using the same model and dataset)**\n",
    "\n",
    "- Using ```num_epochs=40``` and ```batch_size=50```, same augmentation steps and the same hyperparameters the model achieved ```90.1% mAP``` ```88.4% precision``` and ```82.8% recall``` with an average class ```precision``` of ```90%``` on the validation dataset.\n",
    "\n",
    "- We can conclude that increasing the number of epochs and the batch size would lead to better precision and faster training.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/roboflow-platform-cache/RjBpFWbVLQdI2NaOrqg24Eooatr2/qYHiTyjFVuJ6MWIK56Sh/4/results.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ebbd6e-3da3-4881-83dc-06f2cb9ddc84",
   "metadata": {},
   "source": [
    "### Next\n",
    "\n",
    "- Freeze the trained model in correct format for model inferencing\n",
    "- Develop a Jupyter Notebook to perform inference on the frozen model using unseen test images\n",
    "- Discuss my results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlowGPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
