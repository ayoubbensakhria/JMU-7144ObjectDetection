{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4463c0-f797-4245-9ad7-559712c18db1",
   "metadata": {},
   "source": [
    "# 7144COMP/CW2: Bird Multiple Object Detection Using Faster R-CNN \n",
    "## PART 2.Training\n",
    "### Overview\n",
    "In this notebook, I will train an object detection model using the pre-processed data from the previous notebook. \n",
    "\n",
    "- Download the object detection models from Tensorflow 2 Detection Model Zoo >> [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).\n",
    "- The model's hyperparameters and configuration are set in the ```fasterrcnn_config.config``` file. \n",
    "- The model is trained through this notebook using ```model_main_tf2.py``` with the relevent arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37b3612-55a0-4cf5-be10-e9ce65245fd4",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "- Environment Setup (see Part 0)\n",
    "- Data preprocessing (see Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563550b-3c9f-4b45-82c2-c0abfd6e01c1",
   "metadata": {},
   "source": [
    "## 1. Download the model from TensorFlow 2 Detection Model Zoo \n",
    "#### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ac16e330-692f-4f7c-a626-119c1865ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re #<- regular expressions\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b1929-631f-4b0f-a234-717b6f011e6d",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d8807fd-9e1c-4546-9959-f0b9cac5293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "RANDOM_SEED = 99 #<-ensure the reproduciblity of the training results\n",
    "BATCH_SIZE = 1\n",
    "NUM_STEPS = 22000 # <- for 6 epochs (and 400 steps to generate the last ckpt for eval) \n",
    "NUM_EVAL_STEPS = 1000 #<- execute evaluation each 1000 steps\n",
    "\n",
    "# Current directory\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872c116-1e0b-4dce-ba6e-de309d3d3bb7",
   "metadata": {},
   "source": [
    "#### Download Fine-tuned ```Faster R-CNN ResNet101``` from Tensorflow 2 Detection Model Zoo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a33958-a897-462d-856c-357f1e9d41ed",
   "metadata": {},
   "source": [
    "**Why Faster R-CNN**?\n",
    "\n",
    "Faster R-CNN is an object detection model that improves on Fast R-CNN by utilising a region proposal network (RPN) with the CNN model.\n",
    "\n",
    "Faster R-CNN has impressive detection effects in ordinary scenes ([source](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7582940/)).\n",
    "\n",
    "However, under certain conditions, there can still be unsatisfactory detection performance, such as: the object having problems like occlusion, deformation, or small size ([source](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7582940/)).\n",
    "\n",
    "Our project deals with ordinary scenes, according to the requirements, we should prioritise accuracy over speed, therefore, two-step object detectors like Faster R-CNN may be the most suitable for this task given the limitations in terms of time and computing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9267bd88-8fb2-40d4-8483-a6c6c267946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Faster R-CNN ResNet101 if it doesn't exist locally\n",
    "if not os.path.isdir('faster_rcnn_resnet101_v1_640x640_coco17_tpu-8'):\n",
    "    !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.tar.gz\n",
    "    # Unzip and remove compressed files\n",
    "    !tar -xf faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.tar.gz\n",
    "    # Cleanup\n",
    "    !rm faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915eae5c-77c2-4f80-b3fe-d942b5b4a793",
   "metadata": {},
   "source": [
    "#### Load Train, Test, Valid TFRecords, labelmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9b17bb00-d083-45a2-936d-3619d031fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test, Valid TFRecord files\n",
    "train_record_path = os.path.join(current_dir, 'Birds', 'train', 'birds.tfrecord')\n",
    "test_record_path = os.path.join(current_dir, 'Birds', 'test', 'birds.tfrecord')\n",
    "valid_record_path = os.path.join(current_dir, 'Birds', 'valid', 'birds.tfrecord')\n",
    "\n",
    "# Labelmap\n",
    "labelmap_path = os.path.join(current_dir, 'Birds', 'train', 'birds_label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1396c-e1e1-4789-b7a3-add9e25d2356",
   "metadata": {},
   "source": [
    "# 2. Model's Config files, Checkpoints and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cc6bd2e2-57d3-4b04-9dc2-9089c662bdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Dir: faster_rcnn_resnet101_v1_640x640_coco17_tpu-8/checkpoint/ckpt-0\n"
     ]
    }
   ],
   "source": [
    "# Load the latest Checkpoint if it exists\n",
    "fine_tune_checkpoint_fasterrcnn = 'faster_rcnn_resnet101_v1_640x640_coco17_tpu-8/checkpoint/ckpt-0'\n",
    "print('Checkpoint Dir:', fine_tune_checkpoint_fasterrcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40bfea51-4528-4254-b9b1-438939b08e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-06 18:27:19--  https://raw.githubusercontent.com/ayoubbensakhria/TensorFlowOD/master/7144COMP/training/pipeline.config\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3834 (3.7K) [text/plain]\n",
      "Saving to: ‘pipeline.config’\n",
      "\n",
      "pipeline.config     100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-01-06 18:27:20 (108 MB/s) - ‘pipeline.config’ saved [3834/3834]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# config files can be edited and updated on ayoubbensakhria/TensorFlowOD repository\n",
    "if os.path.isfile('pipeline.config'):\n",
    "    !rm 'pipeline.config'\n",
    "\n",
    "# Download the latest base pipeline config file\n",
    "!wget https://raw.githubusercontent.com/ayoubbensakhria/TensorFlowOD/master/7144COMP/training/pipeline.config\n",
    "\n",
    "# data_augmentation_options section has been removed because it has been done by Roboflow\n",
    "base_config_path_fasterrcnn = 'pipeline.config'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91873a86-341a-477c-9183-7f98288807a1",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "Our approach consists of setting a maximum number of epochs (10) and the validation loss as a metric to monitor. The training process will continue until the validation loss has not improved for a certain number of epochs, at which point the training process is stopped.\n",
    "\n",
    "After training at 10 epochs, we observed that validation loss didn't improve after the 6th epoch.\n",
    "\n",
    "```num_epochs=6``` = one forward pass and one backward pass of all the training examples. One step takes on average 2 seconds, an epoch consists of 3600 steps (batch_size=1). \n",
    "\n",
    "However, for most real-world datasets, it may not be sufficient to train a model to good performance, as the model will not have the opportunity to learn from the entire dataset.\n",
    "\n",
    "```batch_size=1``` = the number of training examples in one forward/backward pass (1 step). The higher the batch_size, the more memory space we would need. Here the available memory allows a max of batch_size = 1 \n",
    "\n",
    "```num_steps=21600```: number of iterations, or a single update of the model weights.\n",
    "\n",
    "```fixed_shape_resizer```: a fixed resolution of ```640x640 px``` is useful for ensuring that all input images have the same size, which can make them easier to process and may improve the performance of the model.\n",
    "\n",
    "```grid_anchor_generator```: anchor boxes are used to identify potential object locations within the image. The performance of a Faster R-CNN model can be affected by the parameters of the grid anchor generator such as ```scales```, ```aspect_ratios```, ```height_stride```, ```width_stride```, and it may be necessary to experiment with different values to find the best performing configuration.\n",
    "\n",
    "```second_stage_post_processing```: is responsible for taking the output of the model's second stage (the region proposal network) and generating the final set of object detections. The specific parameters used can have a significant impact on the model's performance.\n",
    "\n",
    " \n",
    "The hyperparameters above may be suitable for quickly testing the performance of a model on our dataset, but they may not be optimal for training a model to good performance on a real-world dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4bbfc0c-07e4-4e3f-92b4-d4aaea92f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config the Model Pipeline Edition function\n",
    "def edit_config(model_name, base_config_path, fine_tune_checkpoint):\n",
    "  with open(base_config_path) as f:\n",
    "    config = f.read()\n",
    "\n",
    "  with open('{model}_config.config'.format(model=model_name), 'w') as f:\n",
    "\n",
    "    # Set labelmap path\n",
    "    config = re.sub('label_map_path: \".*?\"', \n",
    "              'label_map_path: \"{}\"'.format(labelmap_path), config)\n",
    "    \n",
    "    # Set fine_tune_checkpoint path\n",
    "    config = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "                    'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), config)\n",
    "\n",
    "    # Set train tf-record file path\n",
    "    config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', \n",
    "                    'input_path: \"{}\"'.format(train_record_path), config)\n",
    "\n",
    "    # Set test tf-record file path\n",
    "    config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', \n",
    "                    'input_path: \"{}\"'.format(test_record_path), config)\n",
    "\n",
    "    # Set number of classes.\n",
    "    config = re.sub('num_classes: [0-9]+',\n",
    "                    'num_classes: {}'.format(4), config)\n",
    "\n",
    "    # Set batch size\n",
    "    config = re.sub('batch_size: [0-9]+',\n",
    "                    'batch_size: {}'.format(BATCH_SIZE), config)\n",
    "\n",
    "    # Set training steps\n",
    "    config = re.sub('num_steps: [0-9]+',\n",
    "                    'num_steps: {}'.format(NUM_STEPS), config)\n",
    "\n",
    "    # Set fine-tune checkpoint type to detection\n",
    "    config = re.sub('fine_tune_checkpoint_type: \"classification\"', \n",
    "              'fine_tune_checkpoint_type: \"{}\"'.format('detection'), config)\n",
    "\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "759e49af-1222-4db7-9442-3c590c9f4314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Faster R-CNN with Resnet-101 (v1)\n",
      "# Trained on COCO, initialized from Imagenet classification checkpoint\n",
      "\n",
      "# This config is TPU compatible.\n",
      "\n",
      "model {\n",
      "  faster_rcnn {\n",
      "    num_classes: 4\n",
      "    image_resizer {\n",
      "      fixed_shape_resizer {\n",
      "        width: 640\n",
      "        height: 640\n",
      "      }\n",
      "    }\n",
      "    feature_extractor {\n",
      "      type: 'faster_rcnn_resnet101_keras'\n",
      "      batch_norm_trainable: true\n",
      "    }\n",
      "    first_stage_anchor_generator {\n",
      "      grid_anchor_generator {\n",
      "        scales: [0.25, 0.5, 1.0, 2.0]\n",
      "        aspect_ratios: [0.5, 1.0, 2.0]\n",
      "        height_stride: 16\n",
      "        width_stride: 16\n",
      "      }\n",
      "    }\n",
      "    first_stage_box_predictor_conv_hyperparams {\n",
      "      op: CONV\n",
      "      regularizer {\n",
      "        l2_regularizer {\n",
      "          weight: 0.0\n",
      "        }\n",
      "      }\n",
      "      initializer {\n",
      "        truncated_normal_initializer {\n",
      "          stddev: 0.01\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    first_stage_nms_score_threshold: 0.0\n",
      "    first_stage_nms_iou_threshold: 0.7\n",
      "    first_stage_max_proposals: 300\n",
      "    first_stage_localization_loss_weight: 2.0\n",
      "    first_stage_objectness_loss_weight: 1.0\n",
      "    initial_crop_size: 14\n",
      "    maxpool_kernel_size: 2\n",
      "    maxpool_stride: 2\n",
      "    second_stage_box_predictor {\n",
      "      mask_rcnn_box_predictor {\n",
      "        use_dropout: false\n",
      "        dropout_keep_probability: 1.0\n",
      "        fc_hyperparams {\n",
      "          op: FC\n",
      "          regularizer {\n",
      "            l2_regularizer {\n",
      "              weight: 0.0\n",
      "            }\n",
      "          }\n",
      "          initializer {\n",
      "            variance_scaling_initializer {\n",
      "              factor: 1.0\n",
      "              uniform: true\n",
      "              mode: FAN_AVG\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        share_box_across_classes: true\n",
      "      }\n",
      "    }\n",
      "    second_stage_post_processing {\n",
      "      batch_non_max_suppression {\n",
      "        score_threshold: 0.0\n",
      "        iou_threshold: 0.6\n",
      "        max_detections_per_class: 100\n",
      "        max_total_detections: 300\n",
      "      }\n",
      "      score_converter: SOFTMAX\n",
      "    }\n",
      "    second_stage_localization_loss_weight: 2.0\n",
      "    second_stage_classification_loss_weight: 1.0\n",
      "    use_static_shapes: true\n",
      "    use_matmul_crop_and_resize: true\n",
      "    clip_anchors_to_image: true\n",
      "    use_static_balanced_label_sampler: true\n",
      "    use_matmul_gather_in_matcher: true\n",
      "  }\n",
      "}\n",
      "\n",
      "train_config: {\n",
      "  batch_size: 1\n",
      "  sync_replicas: true\n",
      "  startup_delay_steps: 0\n",
      "  replicas_to_aggregate: 8\n",
      "  num_steps: 21600\n",
      "  optimizer {\n",
      "    momentum_optimizer: {\n",
      "      learning_rate: {\n",
      "        cosine_decay_learning_rate {\n",
      "          learning_rate_base: .0004\n",
      "          total_steps: 100000\n",
      "          warmup_learning_rate: .00004\n",
      "          warmup_steps: 2000\n",
      "        }\n",
      "      }\n",
      "      momentum_optimizer_value: 0.9\n",
      "    }\n",
      "    use_moving_average: false\n",
      "  }\n",
      "  fine_tune_checkpoint_version: V2\n",
      "  fine_tune_checkpoint: \"faster_rcnn_resnet101_v1_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n",
      "  fine_tune_checkpoint_type: \"detection\"\n",
      "  data_augmentation_options {\n",
      "    random_horizontal_flip {\n",
      "    }\n",
      "  }\n",
      "\n",
      "  data_augmentation_options {\n",
      "    random_adjust_hue {\n",
      "    }\n",
      "  }\n",
      "\n",
      "  data_augmentation_options {\n",
      "    random_adjust_contrast {\n",
      "    }\n",
      "  }\n",
      "\n",
      "  data_augmentation_options {\n",
      "    random_adjust_saturation {\n",
      "    }\n",
      "  }\n",
      "\n",
      "  data_augmentation_options {\n",
      "     random_square_crop_by_scale {\n",
      "      scale_min: 0.6\n",
      "      scale_max: 1.3\n",
      "    }\n",
      "  }\n",
      "  max_number_of_boxes: 100\n",
      "  unpad_groundtruth_tensors: false\n",
      "  use_bfloat16: false  # works only on TPUs\n",
      "}\n",
      "\n",
      "train_input_reader: {\n",
      "  label_map_path: \"/home/msc1/Desktop/7144COMP/Models/faster_rcnn_resnet101/Birds/train/birds_label_map.pbtxt\"\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"/home/msc1/Desktop/7144COMP/Models/faster_rcnn_resnet101/Birds/train/birds.tfrecord\"\n",
      "  }\n",
      "}\n",
      "\n",
      "eval_config: {\n",
      "  metrics_set: \"coco_detection_metrics\"\n",
      "  use_moving_averages: false\n",
      "  batch_size: 1;\n",
      "}\n",
      "\n",
      "eval_input_reader: {\n",
      "  label_map_path: \"/home/msc1/Desktop/7144COMP/Models/faster_rcnn_resnet101/Birds/train/birds_label_map.pbtxt\"\n",
      "  shuffle: false\n",
      "  num_epochs: 1\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"/home/msc1/Desktop/7144COMP/Models/faster_rcnn_resnet101/Birds/test/birds.tfrecord\"\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "# Edit config Fatser R-CNN\n",
    "edit_config('fasterrcnn', base_config_path_fasterrcnn, fine_tune_checkpoint_fasterrcnn)\n",
    "\n",
    "# Clean up\n",
    "!rm 'pipeline.config'\n",
    "\n",
    "# Print config pipeline\n",
    "%cat 'fasterrcnn_config.config'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4423e2-dc2d-4fc5-b415-411ad8472fa6",
   "metadata": {},
   "source": [
    "# 3. Train Faster R-CNN ResNet101 Object Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b426319-410a-4fe7-9048-2c1fa44fbc34",
   "metadata": {},
   "source": [
    "- The validation script must be run concurrently with the training script in order to visualise the validation loss curve on ```TensorBoard```. \n",
    "- The validation script should be listening to new checkpoints (output: ```Waiting for new checkpoint at``` ...) to execute valiadation each ```1000 steps``` (see part 3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66b12179-4237-4a7a-b6d8-b44af44a4311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasterrcnn_config.config /home/msc1/Desktop/7144COMP/Models/faster_rcnn_resnet101/training 22000\n"
     ]
    }
   ],
   "source": [
    "# Model training directory and config pipeline\n",
    "model_dir = os.path.join(current_dir, 'training')\n",
    "pipeline_config_path = 'fasterrcnn_config.config'\n",
    "\n",
    "# Test training params\n",
    "print (pipeline_config_path, model_dir, NUM_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd290d7-913e-455e-ba84-9f0881eff8f4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 14:34:01.975553: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-08 14:34:02.728050: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-08 14:34:02.728097: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-08 14:34:02.728106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-08 14:34:04.109195: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-01-08 14:34:04.109221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: CMPR01\n",
      "2023-01-08 14:34:04.109228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: CMPR01\n",
      "2023-01-08 14:34:04.109283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.161.3\n",
      "2023-01-08 14:34:04.109302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
      "2023-01-08 14:34:04.109308: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 460.91.3 does not match DSO version 470.161.3 -- cannot find working devices in this configuration\n",
      "2023-01-08 14:34:04.109585: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "W0108 14:34:04.110902 140679829825344 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "I0108 14:34:04.122565 140679829825344 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: 22000\n",
      "I0108 14:34:04.124860 140679829825344 config_util.py:552] Maybe overwriting train_steps: 22000\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I0108 14:34:04.124936 140679829825344 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W0108 14:34:04.151270 140679829825344 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['/home/msc1/Desktop/7144COMP/Models/faster_rcnn_resnet101/Birds/train/birds.tfrecord']\n",
      "I0108 14:34:04.157337 140679829825344 dataset_builder.py:162] Reading unweighted datasets: ['/home/msc1/Desktop/7144COMP/Models/faster_rcnn_resnet101/Birds/train/birds.tfrecord']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/home/msc1/Desktop/7144COMP/Models/faster_rcnn_resnet101/Birds/train/birds.tfrecord']\n",
      "I0108 14:34:04.157462 140679829825344 dataset_builder.py:79] Reading record datasets for input file: ['/home/msc1/Desktop/7144COMP/Models/faster_rcnn_resnet101/Birds/train/birds.tfrecord']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I0108 14:34:04.157518 140679829825344 dataset_builder.py:80] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W0108 14:34:04.157564 140679829825344 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "W0108 14:34:04.162302 140679829825344 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W0108 14:34:04.175789 140679829825344 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "W0108 14:34:04.577886 140679829825344 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W0108 14:34:08.974811 140679829825344 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0108 14:34:12.202368 140679829825344 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "2023-01-08 14:34:14.021127: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "/home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/keras/backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0108 14:34:19.363130 140670303987456 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:459: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "W0108 14:34:24.283555 140670303987456 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:459: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "W0108 14:34:27.628721 140670303987456 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W0108 14:34:39.825581 140670824072960 deprecation.py:554] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "INFO:tensorflow:Step 21100 per-step time 3.183s\n",
      "I0108 14:39:57.842736 140679829825344 model_lib_v2.py:705] Step 21100 per-step time 3.183s\n",
      "INFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.35331494,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.24847347,\n",
      " 'Loss/RPNLoss/localization_loss': 0.09037112,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.0069839717,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.6991435,\n",
      " 'learning_rate': 0.0003636668}\n",
      "I0108 14:39:57.843264 140679829825344 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.35331494,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.24847347,\n",
      " 'Loss/RPNLoss/localization_loss': 0.09037112,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.0069839717,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.6991435,\n",
      " 'learning_rate': 0.0003636668}\n",
      "INFO:tensorflow:Step 21200 per-step time 2.877s\n",
      "I0108 14:44:45.481795 140679829825344 model_lib_v2.py:705] Step 21200 per-step time 2.877s\n",
      "INFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.013794169,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.031874605,\n",
      " 'Loss/RPNLoss/localization_loss': 0.026421182,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.018629536,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.09071949,\n",
      " 'learning_rate': 0.00036329744}\n",
      "I0108 14:44:45.481987 140679829825344 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.013794169,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.031874605,\n",
      " 'Loss/RPNLoss/localization_loss': 0.026421182,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.018629536,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.09071949,\n",
      " 'learning_rate': 0.00036329744}\n",
      "INFO:tensorflow:Step 21300 per-step time 1.933s\n",
      "I0108 14:47:58.801936 140679829825344 model_lib_v2.py:705] Step 21300 per-step time 1.933s\n",
      "INFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.18867245,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.056344014,\n",
      " 'Loss/RPNLoss/localization_loss': 0.011296526,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.0071785236,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.2634915,\n",
      " 'learning_rate': 0.00036292645}\n",
      "I0108 14:47:58.802124 140679829825344 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.18867245,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.056344014,\n",
      " 'Loss/RPNLoss/localization_loss': 0.011296526,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.0071785236,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.2634915,\n",
      " 'learning_rate': 0.00036292645}\n",
      "INFO:tensorflow:Step 21400 per-step time 1.941s\n",
      "I0108 14:51:12.875352 140679829825344 model_lib_v2.py:705] Step 21400 per-step time 1.941s\n",
      "INFO:tensorflow:{'Loss/BoxClassifierLoss/classification_loss': 0.79265183,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.116402104,\n",
      " 'Loss/RPNLoss/localization_loss': 0.0066041276,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.05377739,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.96943545,\n",
      " 'learning_rate': 0.00036255378}\n",
      "I0108 14:51:12.875542 140679829825344 model_lib_v2.py:708] {'Loss/BoxClassifierLoss/classification_loss': 0.79265183,\n",
      " 'Loss/BoxClassifierLoss/localization_loss': 0.116402104,\n",
      " 'Loss/RPNLoss/localization_loss': 0.0066041276,\n",
      " 'Loss/RPNLoss/objectness_loss': 0.05377739,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 0.96943545,\n",
      " 'learning_rate': 0.00036255378}\n"
     ]
    }
   ],
   "source": [
    "# Execute training\n",
    "!python $current_dir/models/research/object_detection/model_main_tf2.py \\\n",
    "    --pipeline_config_path=$pipeline_config_path \\\n",
    "    --model_dir=$model_dir \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps=$NUM_STEPS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae63895-aa91-4ef8-9181-939af6da055d",
   "metadata": {},
   "source": [
    "The script above was used to train an object detection model using ```TensorFlow 2```. It takes in a pipeline configuration file, which specifies the model and training configuration, and a set of training and evaluation data.\n",
    "\n",
    "The script has several flags that can be used to control the training process. \n",
    "\n",
    "- ```--pipeline_config_path```  specifies the path to the pipeline configuration file, which defines the model architecture and training parameters. \n",
    "\n",
    "- ```--model_dir```  specifies the directory where the trained model and training logs should be saved. The --alsologtostderr flag causes the training logs to be written to both the log file and the console.\n",
    "\n",
    "- ```--num_train_steps```  specifies the number of training steps to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07bbbb-12b4-4e45-8ae4-d430c15dce9e",
   "metadata": {},
   "source": [
    "### Export our OD inference graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899df8e-8d20-4265-a9fb-3d244ad1af28",
   "metadata": {},
   "source": [
    "Graphs are data structures that contain a set of ```tf.Operation``` objects, which represent units of computation; and ```tf.Tensor``` objects, which represent the units of data that flow between operations. \n",
    "\n",
    "Here we will save our object detection inference graph files in ```fasterrcnn_inference_graph/saved_model```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10716f79-fc88-453f-9e57-e32b270eaff9",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following script uses the ```exporter_main_v2.py``` script from the TensorFlow object detection library to export the trained model. The script loads the trained model from the specified checkpoint directory and then uses the pipeline configuration file to create a new model (a copy of the trained model) with the same architecture. The exported model is saved in the specified output directory.\n",
    "\n",
    "This new model is a copy of the trained model, but it has been converted to a format that is suitable for serving or for further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "061b0e3a-065c-4ca8-902a-fa7a2e671128",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 14:15:23.888782: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-08 14:15:24.638287: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-08 14:15:24.638336: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-08 14:15:24.638345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-08 14:15:26.156981: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-01-08 14:15:26.157007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: CMPR01\n",
      "2023-01-08 14:15:26.157015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: CMPR01\n",
      "2023-01-08 14:15:26.157068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.161.3\n",
      "2023-01-08 14:15:26.157087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
      "2023-01-08 14:15:26.157094: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 460.91.3 does not match DSO version 470.161.3 -- cannot find working devices in this configuration\n",
      "2023-01-08 14:15:26.175113: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "W0108 14:15:26.233247 140281825490752 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "W0108 14:15:26.293991 140281825490752 deprecation.py:623] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0108 14:15:32.171162 140281825490752 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:459: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "W0108 14:15:37.479371 140281825490752 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:459: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.faster_rcnn_meta_arch.FasterRCNNMetaArch object at 0x7f952986b280>, because it is not built.\n",
      "W0108 14:15:46.320448 140281825490752 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.meta_architectures.faster_rcnn_meta_arch.FasterRCNNMetaArch object at 0x7f952986b280>, because it is not built.\n",
      "W0108 14:16:03.880707 140281825490752 save.py:271] Found untraced functions such as FirstStageBoxPredictor_layer_call_fn, FirstStageBoxPredictor_layer_call_and_return_conditional_losses, mask_rcnn_keras_box_predictor_layer_call_fn, mask_rcnn_keras_box_predictor_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op while saving (showing 5 of 135). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: fasterrcnn_inference_graph/saved_model/assets\n",
      "I0108 14:16:09.960411 140281825490752 builder_impl.py:797] Assets written to: fasterrcnn_inference_graph/saved_model/assets\n",
      "INFO:tensorflow:Writing pipeline config file to fasterrcnn_inference_graph/pipeline.config\n",
      "I0108 14:16:11.195548 140281825490752 config_util.py:253] Writing pipeline config file to fasterrcnn_inference_graph/pipeline.config\n"
     ]
    }
   ],
   "source": [
    "# Define the output directory\n",
    "output_directory = 'fasterrcnn_inference_graph'\n",
    "\n",
    "# Export OD inference graph\n",
    "!python $current_dir/models/research/object_detection/exporter_main_v2.py \\\n",
    "    --trained_checkpoint_dir $model_dir \\\n",
    "    --output_directory $output_directory \\\n",
    "    --pipeline_config_path $pipeline_config_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d0820-e43e-4c36-bd04-8abe984dfa5f",
   "metadata": {},
   "source": [
    "- ``` trained_checkpoint_dir``` : Directory containing the trained model checkpoints.\n",
    "- ``` output_directory``` : Directory where the exported model will be saved.\n",
    "- ``` pipeline_config_path``` : Path to the pipeline configuration file, which specifies the model architecture and other options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f89a5-f50a-43ad-9496-ce255a61f27c",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Evaluate the trained model using TensorBoard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlowGPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
