{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa082d3-1ce9-4020-a41d-9c72aad71756",
   "metadata": {},
   "source": [
    "# 7144COMP/CW2: Bird Multiple Object Detection Using Faster R-CNN ResNet101 Network \n",
    "## PART IV: Model evaluation and deployment\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this notebook, I will evaluate my model through TensorBoard while using the generated metrics to determine model convergence (both validation loss and Intersection over Union (IoU) at both 0.5 and 0.75 are considered). \n",
    "\n",
    "The number of epochs to train the model is set to 1, the reason for this choice was explained in the training notebook. In addition, during the 1st epoch of training, the model converged around the final loss value (smoothed loss value with a weight of 0.8).\n",
    "\n",
    "\n",
    "\n",
    "For the current task, the following steps have been undertaken: \n",
    "\n",
    "- Launch TensorBoard displaying both the train and evaluation metrics for the given session. \n",
    "- Provide justification for the number of epochs used for training your object detection model\n",
    "\n",
    "### Next\n",
    "\n",
    "In the next notebook which is an extension to the present, I will:\n",
    "\n",
    "- Freeze my trained model in correct format for model inferencing\n",
    "- Develop a Jupyter Notebook to perform inference on the frozen model using unseen test images\n",
    "- Discuss my results.\n",
    "\n",
    "### Prerequisites\n",
    "This notebook runs locally on the environment *tf-gpu*.\n",
    "- Environment Setup (see Part 0)\n",
    "- Preprocessing (see Part 1)\n",
    "- Training (see Part 2)\n",
    "- Run the necessary evaluation scripts (see Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a13b846-f597-4ef3-a11c-a87913e96351",
   "metadata": {},
   "source": [
    "## 1. Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3cd42c-70f0-4302-8fc2-5d11fed5b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d3796a-cc91-4ebe-913b-df561cd6d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Model training directory and config pipeline\n",
    "model_dir = os.path.join(current_dir, 'training')\n",
    "pipeline_config_path = 'fasterrcnn_config.config'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b3465-4546-404c-9190-0f751ba09806",
   "metadata": {},
   "source": [
    "## 2. TensorBoard \n",
    "### 2.1. Monitor region proposal losses, evaluation metrics\n",
    "Here ```logdir``` points to the training directory, by launching the next cell, different loss graphs for region proposal network will be imported by TensorBoard from ```training/train``` folder, whereas evaluation metrics for the given session will be imported from the ```training/eval``` folder.\n",
    "\n",
    "The losses for the Region Proposal Network:\n",
    "\n",
    "- ```Loss/RPNLoss/localization_loss```: Localization Loss or the Loss of the Bounding Box regressor for the RPN\n",
    "\n",
    "- ```Loss/RPNLoss/objectness_loss```: Loss of the Classifier that classifies if a bounding box is an object of interest or background\n",
    "\n",
    "The losses for the Final Classifier:\n",
    "\n",
    "- ```Loss/BoxClassifierLoss/classification_loss```: Loss for the classification of detected objects into various classes: Cat, Dog, Airplane etc\n",
    "\n",
    "- ```BoxClassifierLoss/localization_loss```: Localization Loss or the Loss of the Bounding Box regressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32dd5d-575f-4364-8bd1-79fe5ac5bab9",
   "metadata": {},
   "source": [
    "### Display the train and evaluation metrics for the given session \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbae6254-c51b-4c1d-89e8-2bfd56be540c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-01 20:36:36.459974: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-01 20:36:37.108106: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-01 20:36:37.108157: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-01 20:36:37.108166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-01 20:36:37.888622: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-01-01 20:36:37.888646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: CMPR01\n",
      "2023-01-01 20:36:37.888653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: CMPR01\n",
      "2023-01-01 20:36:37.888704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.161.3\n",
      "2023-01-01 20:36:37.888724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
      "2023-01-01 20:36:37.888731: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 460.91.3 does not match DSO version 470.161.3 -- cannot find working devices in this configuration\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.11.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir $current_dir'/training/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bdc29b-be80-4080-8bf1-9abdabf2345c",
   "metadata": {},
   "source": [
    "## 3. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73404608-f3d5-4843-bf84-224741185833",
   "metadata": {},
   "source": [
    "### Training vs Validation losses\n",
    "\n",
    "- The training total loss was ```train_total_loss = 0.1784```, whereas the validation total loss was ```val_total_loss = 0.2659``` at the last step of training. \n",
    "- Both losses seem to converge down towards lower error levels, which is a good sign that the model is learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ae06a-2007-4b3c-8c11-a48778846d76",
   "metadata": {},
   "source": [
    "### Precision Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4ab9a-2d0e-4759-9c16-f57f50965d9a",
   "metadata": {},
   "source": [
    "<table>\n",
    "\t<thead>\n",
    "\t\t<tr>\n",
    "\t\t\t<th>Metric</th>\n",
    "\t\t\t<th>Value</th>\n",
    "\t\t\t<th>Interpretation</th>\n",
    "\t\t</tr>\n",
    "\t</thead>\n",
    "\t<tbody>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP)</td>\n",
    "\t\t\t<td>0.549</td>\n",
    "\t\t\t<td>The model&#39;s average precision across all object sizes and IoU thresholds is 0.549</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) @ IoU=0.50</td>\n",
    "\t\t\t<td>0.871</td>\n",
    "\t\t\t<td>The model&#39;s average precision with an IoU threshold of 0.50 is 0.871</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) @ IoU=0.75</td>\n",
    "\t\t\t<td>0.642</td>\n",
    "\t\t\t<td>The model&#39;s average precision with an IoU threshold of 0.75 is 0.642</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) (small)</td>\n",
    "\t\t\t<td>0.177</td>\n",
    "\t\t\t<td>The model&#39;s average precision for small objects is 0.177</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) (medium)</td>\n",
    "\t\t\t<td>0.419</td>\n",
    "\t\t\t<td>The model&#39;s average precision for medium-sized objects is 0.419</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Precision (AP) (large)</td>\n",
    "\t\t\t<td>0.623</td>\n",
    "\t\t\t<td>The model&#39;s average precision for large objects is 0.623</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) @ maxDets=1</td>\n",
    "\t\t\t<td>0.604</td>\n",
    "\t\t\t<td>The model&#39;s average recall with a maximum of 1 detection per image is 0.604</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) @ maxDets=10</td>\n",
    "\t\t\t<td>0.651</td>\n",
    "\t\t\t<td>The model&#39;s average recall with a maximum of 10 detections per image is 0.651</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) @ maxDets=100</td>\n",
    "\t\t\t<td>0.654</td>\n",
    "\t\t\t<td>The model&#39;s average recall with a maximum of 100 detections per image is 0.654</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) (small)</td>\n",
    "\t\t\t<td>0.244</td>\n",
    "\t\t\t<td>The model&#39;s average recall for small objects is 0.244</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) (medium)</td>\n",
    "\t\t\t<td>0.555</td>\n",
    "\t\t\t<td>The model&#39;s average recall for medium-sized objects is 0.555</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Average Recall (AR) (large)</td>\n",
    "\t\t\t<td>0.714</td>\n",
    "\t\t\t<td>The model&#39;s average recall for large objects is 0.714</td>\n",
    "\t\t</tr>\n",
    "\t</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dbd66d-0b41-441f-a07c-928e0c7ed5d0",
   "metadata": {},
   "source": [
    "- The model's performance for object detection is average, with an average precision of ```0.549``` and an average recall of ```0.654```. The average precision indicates the model's ability to correctly identify objects and avoid false positives, while the average recall indicates its ability to detect all instances of the objects in the image.\n",
    "\n",
    "- The model performs particularly well in terms of precision for objects with an ```IoU=0.50``` (intersection over union), which means that the predicted bounding box for the object has a ```50%``` overlap with the ground truth bounding box. The model's precision for this IoU threshold is ```0.871```. However, the model performs less well for objects with an ```IoU=0.75```, with a precision of ```0.642```.\n",
    "\n",
    "- In terms of recall, the model performs better for larger objects, with a value of ```0.714``` compared to ```0.244``` for small objects. This means that the model is more successful at detecting large objects in the image, although there is still room for improvement in detecting small objects.\n",
    "\n",
    "### Justification for the number of epochs used for training your object detection model\n",
    "\n",
    "```num_epochs``` : A number of epochs of ```1``` means that the model will only make one pass through the entire training dataset. Since we'd like to quickly test the performance of the model on a small training data. However, for most real-world datasets, it is generally not sufficient to train a model to good performance, as the model will not have the opportunity to learn from the entire dataset.\n",
    "\n",
    "While it is generally not recommended to train an object detection model using only one epoch, our purpose is to test the model's ```pretrained weights``` used as a starting point in training and our ```hyperparameters config``` on a relatively small dataset of 4000 images. This is due to the limitation in time and resources.\n",
    "\n",
    "Increasing the number of epochs (num_epochs) during training can help improve the performance of an object detection model in a few different ways:\n",
    "\n",
    "- **Increased model convergence**: by allowing the model to see the training data more times, which can help it learn more effectively and converge on a better solution.\n",
    "\n",
    "- **Improved generalisation**: A model that has been trained for more epochs may be better able to generalize to new, unseen data. This is because the model has been exposed to more diverse examples during training and has had more opportunities to learn about the underlying patterns in the data.\n",
    "\n",
    "- **More time for optimisation**: Training for more epochs gives the model more time to adjust its weights and biases through the optimization process. This can lead to improved model performance, especially if the learning rate is set appropriately.\n",
    "\n",
    "Nevertheless, increasing the number of epochs also increases the training time and can lead to overfitting if the model is trained for too many epochs. Overfitting occurs when a model becomes *too specialized to the training data and performs poorly on new, unseen data*.\n",
    "\n",
    "Note: It was not possible to increase the batch size due to memory limitations. \n",
    "\n",
    "#### **Comparison with Roboflow Cloud-based experiment (using the same model and dataset)**\n",
    "\n",
    "- Using ```num_epochs=40``` and ```batch_size=50```, same augmentation steps and the same hyperparameters the model achieved ```90.1% mAP``` ```88.4% precision``` and ```82.8% recall``` with an average class ```precision``` of ```90%``` on the validation dataset.\n",
    "\n",
    "- We can conclude that increasing the number of epochs and the batch size would lead to better precision and faster training.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/roboflow-platform-cache/RjBpFWbVLQdI2NaOrqg24Eooatr2/qYHiTyjFVuJ6MWIK56Sh/4/results.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ebbd6e-3da3-4881-83dc-06f2cb9ddc84",
   "metadata": {},
   "source": [
    "### Next\n",
    "\n",
    "- Freeze the trained model in correct format for model inferencing\n",
    "- Develop a Jupyter Notebook to perform inference on the frozen model using unseen test images\n",
    "- Discuss my results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlowGPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
