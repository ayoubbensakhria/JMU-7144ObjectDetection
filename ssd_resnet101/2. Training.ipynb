{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4463c0-f797-4245-9ad7-559712c18db1",
   "metadata": {},
   "source": [
    "# 7144COMP/CW2: Bird Multiple Object Detection Using SSD\n",
    "## PART 2.Training\n",
    "### Overview\n",
    "In this notebook, I will train an object detection model using the pre-processed data from the previous notebook. \n",
    "\n",
    "- Download the object detection models from Tensorflow 2 Detection Model Zoo >> [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).\n",
    "- The model's hyperparameters and configuration are set in the ```ssd_config.config``` file. \n",
    "- The model is trained through this notebook using ```model_main_tf2.py``` with the relevent arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37b3612-55a0-4cf5-be10-e9ce65245fd4",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "- Environment Setup (see Part 0)\n",
    "- Data preprocessing (see Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563550b-3c9f-4b45-82c2-c0abfd6e01c1",
   "metadata": {},
   "source": [
    "## 1. Download the model from TensorFlow 2 Detection Model Zoo \n",
    "#### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac16e330-692f-4f7c-a626-119c1865ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re #<- regular expressions\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b1929-631f-4b0f-a234-717b6f011e6d",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d8807fd-9e1c-4546-9959-f0b9cac5293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "RANDOM_SEED = 99 #<-ensure the reproduciblity of the training results\n",
    "BATCH_SIZE = 1\n",
    "NUM_STEPS = 28000 \n",
    "NUM_EVAL_STEPS = 1000\n",
    "EPOCHS = 1\n",
    "\n",
    "# Current directory\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872c116-1e0b-4dce-ba6e-de309d3d3bb7",
   "metadata": {},
   "source": [
    "#### Download Fine-tuned ```SSD ResNet101``` from Tensorflow 2 Detection Model Zoo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a33958-a897-462d-856c-357f1e9d41ed",
   "metadata": {},
   "source": [
    "**Why SSD**?\n",
    "\n",
    "Single Shot Multibox Detector (SSD) is a fast and accurate object detection algorithm that uses a single deep neural network to predict object classes and locations. It was developed by researchers at Google and published in a 2015 paper titled \"SSD: Single Shot MultiBox Detector.\"\n",
    "\n",
    "One advantage of SSD is that it is relatively fast compared to other object detection algorithms, as it uses a **single feedforward convolutional neural network (CNN)** to make predictions. This allows it to run in real-time on most devices.\n",
    "\n",
    "Another advantage of SSD is that it is relatively simple to implement and train, as it does not require the use of region proposal algorithms or anchor boxes.\n",
    "\n",
    "A disadvantage of SSD is that it may not be as accurate as other object detection algorithms, such as Faster R-CNN, which uses a two-stage approach to object detection. \n",
    "\n",
    "However, recent improvements to the SSD architecture, such as the addition of the ```ResNet101``` feature extractor, have significantly improved the accuracy of SSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9267bd88-8fb2-40d4-8483-a6c6c267946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SSD ResNet101 if it doesn't exist locally\n",
    "if not os.path.isdir('ssd_resnet101_v1_fpn_640x640_coco17_tpu-8'):\n",
    "    !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "    # Decompression and remove compressed files\n",
    "    !tar -xf ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "    # Cleanup\n",
    "    !rm ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915eae5c-77c2-4f80-b3fe-d942b5b4a793",
   "metadata": {},
   "source": [
    "#### Load Train, Test, Valid TFRecords, labelmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b17bb00-d083-45a2-936d-3619d031fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test, Valid TFRecord files\n",
    "train_record_path = os.path.join(current_dir, 'Birds', 'train', 'birds.tfrecord')\n",
    "test_record_path = os.path.join(current_dir, 'Birds', 'test', 'birds.tfrecord')\n",
    "valid_record_path = os.path.join(current_dir, 'Birds', 'valid', 'birds.tfrecord')\n",
    "\n",
    "# Labelmap\n",
    "labelmap_path = os.path.join(current_dir, 'Birds', 'train', 'birds_label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1396c-e1e1-4789-b7a3-add9e25d2356",
   "metadata": {},
   "source": [
    "# 2. Model's Config files, Checkpoints and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc6bd2e2-57d3-4b04-9dc2-9089c662bdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Dir: ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0\n"
     ]
    }
   ],
   "source": [
    "# Load the latest Checkpoint if it exists\n",
    "fine_tune_checkpoint_ssd = 'ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0'\n",
    "print('Checkpoint Dir:', fine_tune_checkpoint_ssd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40bfea51-4528-4254-b9b1-438939b08e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-03 21:24:59--  https://raw.githubusercontent.com/ayoubbensakhria/TensorFlowOD/master/7144COMP/training/ssd_pipeline.config\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4921 (4.8K) [text/plain]\n",
      "Saving to: ‘ssd_pipeline.config’\n",
      "\n",
      "ssd_pipeline.config 100%[===================>]   4.81K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-01-03 21:24:59 (114 MB/s) - ‘ssd_pipeline.config’ saved [4921/4921]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# config files can be edited and updated on ayoubbensakhria/TensorFlowOD repository\n",
    "if os.path.isfile('ssd_pipeline.config'):\n",
    "    !rm 'ssd_pipeline.config'\n",
    "\n",
    "# Download the latest base pipeline config file\n",
    "!wget https://raw.githubusercontent.com/ayoubbensakhria/TensorFlowOD/master/7144COMP/training/ssd_pipeline.config\n",
    "\n",
    "# data_augmentation_options section has been removed because it has been done by Roboflow\n",
    "base_config_path_ssd = 'ssd_pipeline.config'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91873a86-341a-477c-9183-7f98288807a1",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "```num_epochs``` = one forward pass and one backward pass of all the training examples. One step takes on average 2 seconds, an epoch consists of 28000 steps (batch_size=1), so the total duration of an epoch is approx 15 hours. \n",
    "\n",
    "A number of epochs of ```1``` means that the model will only make one pass through the entire training dataset. Since we'd like to quickly test the performance of the model on a small training data. However, for most real-world datasets, it is generally not sufficient to train a model to good performance, as the model will not have the opportunity to learn from the entire dataset.\n",
    "\n",
    "```batch_size``` = the number of training examples in one forward/backward pass (1 step). The higher the batch_size, the more memory space we would need. Here the available memory allows a max of batch_size = 1 \n",
    "\n",
    "A batch size of ```1``` means that the model will process and update its weights based on a single example at a time. This can be useful in our case where we are trying to train on a very small dataset, as it allows the model to make more frequent weight updates. However, it can also be inefficient and slow, as the model must process and update the weights for each example individually.\n",
    "\n",
    "```num_steps```: number of iterations, or a single update of the model weights \n",
    "\n",
    "The number of steps is equal to the number of examples in the dataset (4000) divided by the batch size (1), so it is reasonable to set the number of steps to ```28000``` in this case. However, it is worth noting that the number of steps should generally be determined based on the number of epochs and the batch size, rather than the total number of examples in the dataset.\n",
    "\n",
    "```fixed_shape_resizer```: a fixed resolution of ```640x640 px``` is useful for ensuring that all input images have the same size, which can make them easier to process and may improve the performance of the model.\n",
    "\n",
    "```grid_anchor_generator```: anchor boxes are used to identify potential object locations within the image. The performance of a Faster R-CNN model can be affected by the parameters of the grid anchor generator such as ```scales```, ```aspect_ratios```, ```height_stride```, ```width_stride```, and it may be necessary to experiment with different values to find the best performing configuration.\n",
    "\n",
    "```second_stage_post_processing```: is responsible for taking the output of the model's second stage (the region proposal network) and generating the final set of object detections. The specific parameters used can have a significant impact on the model's performance.\n",
    "\n",
    " \n",
    "Overall, these hyperparameters may be suitable for quickly testing the performance of a model on our dataset, but they may not be optimal for training a model to good performance on a real-world dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4bbfc0c-07e4-4e3f-92b4-d4aaea92f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config the Model Pipeline Edition function\n",
    "def edit_config(model_name, base_config_path, fine_tune_checkpoint):\n",
    "  with open(base_config_path) as f:\n",
    "    config = f.read()\n",
    "\n",
    "  with open('{model}_config.config'.format(model=model_name), 'w') as f:\n",
    "\n",
    "    # Set labelmap path\n",
    "    config = re.sub('label_map_path: \".*£?\"', \n",
    "              'label_map_path: \"{}\"'.format(labelmap_path), config)\n",
    "    \n",
    "    # Set fine_tune_checkpoint path\n",
    "    config = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "                    'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), config)\n",
    "\n",
    "    # Set train tf-record file path\n",
    "    config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', \n",
    "                    'input_path: \"{}\"'.format(train_record_path), config)\n",
    "\n",
    "    # Set test tf-record file path\n",
    "    config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', \n",
    "                    'input_path: \"{}\"'.format(test_record_path), config)\n",
    "\n",
    "    # Set number of classes.\n",
    "    config = re.sub('num_classes: [0-9]+',\n",
    "                    'num_classes: {}'.format(4), config)\n",
    "\n",
    "    # Set batch size\n",
    "    config = re.sub('batch_size: [0-9]+',\n",
    "                    'batch_size: {}'.format(BATCH_SIZE), config)\n",
    "\n",
    "    # Set training steps\n",
    "    config = re.sub('num_steps: [0-9]+',\n",
    "                    'num_steps: {}'.format(NUM_STEPS), config)\n",
    "\n",
    "    # Set fine-tune checkpoint type to detection\n",
    "    config = re.sub('fine_tune_checkpoint_type: \"classification\"', \n",
    "              'fine_tune_checkpoint_type: \"{}\"'.format('detection'), config)\n",
    "\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "759e49af-1222-4db7-9442-3c590c9f4314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# SSD with Resnet 101 v1 FPN feature extractor, shared box predictor and focal\n",
      "# loss (a.k.a Retinanet).\n",
      "# See Lin et al, https://arxiv.org/abs/1708.02002\n",
      "# Trained on COCO, initialized from Imagenet classification checkpoint\n",
      "# Train on TPU-8\n",
      "#\n",
      "# Achieves 35.4 mAP on COCO17 Val\n",
      "\n",
      "model {\n",
      "  ssd {\n",
      "    inplace_batchnorm_update: true\n",
      "    freeze_batchnorm: false\n",
      "    num_classes: 4\n",
      "    box_coder {\n",
      "      faster_rcnn_box_coder {\n",
      "        y_scale: 10.0\n",
      "        x_scale: 10.0\n",
      "        height_scale: 5.0\n",
      "        width_scale: 5.0\n",
      "      }\n",
      "    }\n",
      "    matcher {\n",
      "      argmax_matcher {\n",
      "        matched_threshold: 0.5\n",
      "        unmatched_threshold: 0.5\n",
      "        ignore_thresholds: false\n",
      "        negatives_lower_than_unmatched: true\n",
      "        force_match_for_each_row: true\n",
      "        use_matmul_gather: true\n",
      "      }\n",
      "    }\n",
      "    similarity_calculator {\n",
      "      iou_similarity {\n",
      "      }\n",
      "    }\n",
      "    encode_background_as_zeros: true\n",
      "    anchor_generator {\n",
      "      multiscale_anchor_generator {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "        anchor_scale: 4.0\n",
      "        aspect_ratios: [1.0, 2.0, 0.5]\n",
      "        scales_per_octave: 2\n",
      "      }\n",
      "    }\n",
      "    image_resizer {\n",
      "      fixed_shape_resizer {\n",
      "        height: 640\n",
      "        width: 640\n",
      "      }\n",
      "    }\n",
      "    box_predictor {\n",
      "      weight_shared_convolutional_box_predictor {\n",
      "        depth: 256\n",
      "        class_prediction_bias_init: -4.6\n",
      "        conv_hyperparams {\n",
      "          activation: RELU_6,\n",
      "          regularizer {\n",
      "            l2_regularizer {\n",
      "              weight: 0.0\n",
      "            }\n",
      "          }\n",
      "          initializer {\n",
      "            random_normal_initializer {\n",
      "              stddev: 0.01\n",
      "              mean: 0.0\n",
      "            }\n",
      "          }\n",
      "          batch_norm {\n",
      "            scale: true,\n",
      "            decay: 0.997,\n",
      "            epsilon: 0.001,\n",
      "          }\n",
      "        }\n",
      "        num_layers_before_predictor: 4\n",
      "        kernel_size: 3\n",
      "      }\n",
      "    }\n",
      "    feature_extractor {\n",
      "      type: 'ssd_resnet101_v1_fpn_keras'\n",
      "      fpn {\n",
      "        min_level: 3\n",
      "        max_level: 7\n",
      "      }\n",
      "      min_depth: 16\n",
      "      depth_multiplier: 1.0\n",
      "      conv_hyperparams {\n",
      "        activation: RELU_6,\n",
      "        regularizer {\n",
      "          l2_regularizer {\n",
      "            weight: 0.0\n",
      "          }\n",
      "        }\n",
      "        initializer {\n",
      "          truncated_normal_initializer {\n",
      "            stddev: 0.03\n",
      "            mean: 0.0\n",
      "          }\n",
      "        }\n",
      "        batch_norm {\n",
      "          scale: true,\n",
      "          decay: 0.997,\n",
      "          epsilon: 0.001,\n",
      "        }\n",
      "      }\n",
      "      override_base_feature_extractor_hyperparams: true\n",
      "    }\n",
      "    loss {\n",
      "      classification_loss {\n",
      "        weighted_sigmoid_focal {\n",
      "          alpha: 0.25\n",
      "          gamma: 2.0\n",
      "        }\n",
      "      }\n",
      "      localization_loss {\n",
      "        weighted_smooth_l1 {\n",
      "        }\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 1.0\n",
      "    }\n",
      "    normalize_loss_by_num_matches: true\n",
      "    normalize_loc_loss_by_codesize: true\n",
      "    post_processing {\n",
      "      batch_non_max_suppression {\n",
      "        score_threshold: 1e-8\n",
      "        iou_threshold: 0.6\n",
      "        max_detections_per_class: 100\n",
      "        max_total_detections: 100\n",
      "      }\n",
      "      score_converter: SIGMOID\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "train_config: {\n",
      "  fine_tune_checkpoint_version: V2\n",
      "  fine_tune_checkpoint: \"ssd_resnet101_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n",
      "  fine_tune_checkpoint_type: \"detection\"\n",
      "  batch_size: 1\n",
      "  sync_replicas: true\n",
      "  startup_delay_steps: 0\n",
      "  replicas_to_aggregate: 8\n",
      "  use_bfloat16: true\n",
      "  num_steps: 28000\n",
      "  data_augmentation_options {\n",
      "    random_horizontal_flip {\n",
      "    }\n",
      "  }\n",
      "  data_augmentation_options {\n",
      "    random_crop_image {\n",
      "      min_object_covered: 0.0\n",
      "      min_aspect_ratio: 0.75\n",
      "      max_aspect_ratio: 3.0\n",
      "      min_area: 0.75\n",
      "      max_area: 1.0\n",
      "      overlap_thresh: 0.0\n",
      "    }\n",
      "  }\n",
      "  optimizer {\n",
      "    momentum_optimizer: {\n",
      "      learning_rate: {\n",
      "        cosine_decay_learning_rate {\n",
      "          learning_rate_base: .04\n",
      "          total_steps: 25000\n",
      "          warmup_learning_rate: .013333\n",
      "          warmup_steps: 2000\n",
      "        }\n",
      "      }\n",
      "      momentum_optimizer_value: 0.9\n",
      "    }\n",
      "    use_moving_average: false\n",
      "  }\n",
      "    data_augmentation_options {\n",
      "    random_horizontal_flip {\n",
      "    }\n",
      "  }\n",
      "\n",
      "  data_augmentation_options {\n",
      "    random_adjust_hue {\n",
      "    }\n",
      "  }\n",
      "\n",
      "  data_augmentation_options {\n",
      "    random_adjust_contrast {\n",
      "    }\n",
      "  }\n",
      "\n",
      "  data_augmentation_options {\n",
      "    random_adjust_saturation {\n",
      "    }\n",
      "  }\n",
      "\n",
      "  data_augmentation_options {\n",
      "     random_square_crop_by_scale {\n",
      "      scale_min: 0.6\n",
      "      scale_max: 1.3\n",
      "    }\n",
      "  }\n",
      "  max_number_of_boxes: 100\n",
      "  unpad_groundtruth_tensors: false\n",
      "}\n",
      "\n",
      "train_input_reader: {\n",
      "  label_map_path: \"/home/msc1/Desktop/7144COMP/Models/ssd_resnet101/Birds/train/birds_label_map.pbtxt\"\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"/home/msc1/Desktop/7144COMP/Models/ssd_resnet101/Birds/train/birds.tfrecord\"\n",
      "  }\n",
      "}\n",
      "\n",
      "eval_config: {\n",
      "  metrics_set: \"coco_detection_metrics\"\n",
      "  use_moving_averages: false\n",
      "}\n",
      "\n",
      "eval_input_reader: {\n",
      "  label_map_path: \"/home/msc1/Desktop/7144COMP/Models/ssd_resnet101/Birds/train/birds_label_map.pbtxt\"\n",
      "  shuffle: false\n",
      "  num_epochs: 1\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"/home/msc1/Desktop/7144COMP/Models/ssd_resnet101/Birds/test/birds.tfrecord\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Edit config SSD\n",
    "edit_config('ssd', base_config_path_ssd, fine_tune_checkpoint_ssd)\n",
    "\n",
    "# Clean up\n",
    "!rm 'ssd_pipeline.config'\n",
    "\n",
    "# Print config pipeline\n",
    "%cat 'ssd_config.config'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4423e2-dc2d-4fc5-b415-411ad8472fa6",
   "metadata": {},
   "source": [
    "## 3. Train SSD ResNet101 Object Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66b12179-4237-4a7a-b6d8-b44af44a4311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssd_config.config /home/msc1/Desktop/7144COMP/Models/ssd_resnet101/training 28000\n"
     ]
    }
   ],
   "source": [
    "# Model training directory and config pipeline\n",
    "model_dir = os.path.join(current_dir, 'training')\n",
    "pipeline_config_path = 'ssd_config.config'\n",
    "\n",
    "# Test training params\n",
    "print (pipeline_config_path, model_dir, NUM_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd290d7-913e-455e-ba84-9f0881eff8f4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03 21:25:23.738115: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-03 21:25:24.771567: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-03 21:25:24.771629: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-03 21:25:24.771637: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-03 21:25:26.605340: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-01-03 21:25:26.605365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: CMPR01\n",
      "2023-01-03 21:25:26.605372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: CMPR01\n",
      "2023-01-03 21:25:26.605433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.161.3\n",
      "2023-01-03 21:25:26.605452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
      "2023-01-03 21:25:26.605459: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 460.91.3 does not match DSO version 470.161.3 -- cannot find working devices in this configuration\n",
      "2023-01-03 21:25:26.605772: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "W0103 21:25:26.607131 139788571789120 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "I0103 21:25:26.620092 139788571789120 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: 28000\n",
      "I0103 21:25:26.623142 139788571789120 config_util.py:552] Maybe overwriting train_steps: 28000\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I0103 21:25:26.623226 139788571789120 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W0103 21:25:26.642810 139788571789120 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['/home/msc1/Desktop/7144COMP/Models/ssd_resnet101/Birds/train/birds.tfrecord']\n",
      "I0103 21:25:26.649168 139788571789120 dataset_builder.py:162] Reading unweighted datasets: ['/home/msc1/Desktop/7144COMP/Models/ssd_resnet101/Birds/train/birds.tfrecord']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/home/msc1/Desktop/7144COMP/Models/ssd_resnet101/Birds/train/birds.tfrecord']\n",
      "I0103 21:25:26.649323 139788571789120 dataset_builder.py:79] Reading record datasets for input file: ['/home/msc1/Desktop/7144COMP/Models/ssd_resnet101/Birds/train/birds.tfrecord']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I0103 21:25:26.649389 139788571789120 dataset_builder.py:80] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W0103 21:25:26.649437 139788571789120 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "W0103 21:25:26.654095 139788571789120 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W0103 21:25:26.668300 139788571789120 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "W0103 21:25:27.091740 139788571789120 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W0103 21:25:31.531007 139788571789120 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "W0103 21:25:33.781740 139788571789120 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0103 21:25:35.935255 139788571789120 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "2023-01-03 21:25:37.829567: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "/home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/keras/backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W0103 21:25:58.323099 139779433817856 deprecation.py:554] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "INFO:tensorflow:Step 100 per-step time 2.289s\n",
      "I0103 21:29:46.812705 139788571789120 model_lib_v2.py:705] Step 100 per-step time 2.289s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 1.2625983,\n",
      " 'Loss/localization_loss': 1.9009223,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 3.1635206,\n",
      " 'learning_rate': 0.014666351}\n",
      "I0103 21:29:46.812896 139788571789120 model_lib_v2.py:708] {'Loss/classification_loss': 1.2625983,\n",
      " 'Loss/localization_loss': 1.9009223,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 3.1635206,\n",
      " 'learning_rate': 0.014666351}\n",
      "INFO:tensorflow:Step 200 per-step time 1.971s\n",
      "I0103 21:33:03.894682 139788571789120 model_lib_v2.py:705] Step 200 per-step time 1.971s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 1.5961901,\n",
      " 'Loss/localization_loss': 1.1756048,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 2.7717948,\n",
      " 'learning_rate': 0.0159997}\n",
      "I0103 21:33:03.894860 139788571789120 model_lib_v2.py:708] {'Loss/classification_loss': 1.5961901,\n",
      " 'Loss/localization_loss': 1.1756048,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 2.7717948,\n",
      " 'learning_rate': 0.0159997}\n",
      "INFO:tensorflow:Step 300 per-step time 1.925s\n",
      "I0103 21:36:16.381688 139788571789120 model_lib_v2.py:705] Step 300 per-step time 1.925s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 1.622343,\n",
      " 'Loss/localization_loss': 0.7698732,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 2.3922162,\n",
      " 'learning_rate': 0.01733305}\n",
      "I0103 21:36:16.381866 139788571789120 model_lib_v2.py:708] {'Loss/classification_loss': 1.622343,\n",
      " 'Loss/localization_loss': 0.7698732,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 2.3922162,\n",
      " 'learning_rate': 0.01733305}\n",
      "INFO:tensorflow:Step 400 per-step time 1.918s\n",
      "I0103 21:39:28.216151 139788571789120 model_lib_v2.py:705] Step 400 per-step time 1.918s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 1.7270912,\n",
      " 'Loss/localization_loss': 0.9734666,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 2.7005577,\n",
      " 'learning_rate': 0.0186664}\n",
      "I0103 21:39:28.216331 139788571789120 model_lib_v2.py:708] {'Loss/classification_loss': 1.7270912,\n",
      " 'Loss/localization_loss': 0.9734666,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 2.7005577,\n",
      " 'learning_rate': 0.0186664}\n",
      "INFO:tensorflow:Step 500 per-step time 1.913s\n",
      "I0103 21:42:39.499921 139788571789120 model_lib_v2.py:705] Step 500 per-step time 1.913s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.9709832,\n",
      " 'Loss/localization_loss': 0.6378126,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.6087959,\n",
      " 'learning_rate': 0.01999975}\n",
      "I0103 21:42:39.500096 139788571789120 model_lib_v2.py:708] {'Loss/classification_loss': 0.9709832,\n",
      " 'Loss/localization_loss': 0.6378126,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.6087959,\n",
      " 'learning_rate': 0.01999975}\n",
      "INFO:tensorflow:Step 600 per-step time 1.927s\n",
      "I0103 21:45:52.224950 139788571789120 model_lib_v2.py:705] Step 600 per-step time 1.927s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 1.9991552,\n",
      " 'Loss/localization_loss': 0.9452014,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 2.9443564,\n",
      " 'learning_rate': 0.0213331}\n",
      "I0103 21:45:52.225125 139788571789120 model_lib_v2.py:708] {'Loss/classification_loss': 1.9991552,\n",
      " 'Loss/localization_loss': 0.9452014,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 2.9443564,\n",
      " 'learning_rate': 0.0213331}\n",
      "INFO:tensorflow:Step 700 per-step time 1.932s\n",
      "I0103 21:49:05.437208 139788571789120 model_lib_v2.py:705] Step 700 per-step time 1.932s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 1.4238573,\n",
      " 'Loss/localization_loss': 0.78006995,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 2.2039273,\n",
      " 'learning_rate': 0.02266645}\n",
      "I0103 21:49:05.437384 139788571789120 model_lib_v2.py:708] {'Loss/classification_loss': 1.4238573,\n",
      " 'Loss/localization_loss': 0.78006995,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 2.2039273,\n",
      " 'learning_rate': 0.02266645}\n",
      "INFO:tensorflow:Step 800 per-step time 1.944s\n",
      "I0103 21:52:19.834248 139788571789120 model_lib_v2.py:705] Step 800 per-step time 1.944s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.691002,\n",
      " 'Loss/localization_loss': 0.600169,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.2911711,\n",
      " 'learning_rate': 0.023999799}\n",
      "I0103 21:52:19.834422 139788571789120 model_lib_v2.py:708] {'Loss/classification_loss': 0.691002,\n",
      " 'Loss/localization_loss': 0.600169,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.2911711,\n",
      " 'learning_rate': 0.023999799}\n",
      "INFO:tensorflow:Step 900 per-step time 1.922s\n",
      "I0103 21:55:32.000509 139788571789120 model_lib_v2.py:705] Step 900 per-step time 1.922s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.9779229,\n",
      " 'Loss/localization_loss': 0.61128914,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.5892121,\n",
      " 'learning_rate': 0.025333151}\n",
      "I0103 21:55:32.000684 139788571789120 model_lib_v2.py:708] {'Loss/classification_loss': 0.9779229,\n",
      " 'Loss/localization_loss': 0.61128914,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.5892121,\n",
      " 'learning_rate': 0.025333151}\n",
      "INFO:tensorflow:Step 1000 per-step time 1.928s\n",
      "I0103 21:58:44.751782 139788571789120 model_lib_v2.py:705] Step 1000 per-step time 1.928s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.96129465,\n",
      " 'Loss/localization_loss': 0.72526187,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.6865566,\n",
      " 'learning_rate': 0.0266665}\n",
      "I0103 21:58:44.751956 139788571789120 model_lib_v2.py:708] {'Loss/classification_loss': 0.96129465,\n",
      " 'Loss/localization_loss': 0.72526187,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.6865566,\n",
      " 'learning_rate': 0.0266665}\n",
      "INFO:tensorflow:Step 1100 per-step time 1.942s\n",
      "I0103 22:01:58.930932 139788571789120 model_lib_v2.py:705] Step 1100 per-step time 1.942s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.9645707,\n",
      " 'Loss/localization_loss': 0.6415433,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.606114,\n",
      " 'learning_rate': 0.02799985}\n",
      "I0103 22:01:58.931126 139788571789120 model_lib_v2.py:708] {'Loss/classification_loss': 0.9645707,\n",
      " 'Loss/localization_loss': 0.6415433,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.606114,\n",
      " 'learning_rate': 0.02799985}\n",
      "INFO:tensorflow:Step 1200 per-step time 1.934s\n",
      "I0103 22:05:12.288000 139788571789120 model_lib_v2.py:705] Step 1200 per-step time 1.934s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.82894284,\n",
      " 'Loss/localization_loss': 0.6369645,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.4659073,\n",
      " 'learning_rate': 0.0293332}\n",
      "I0103 22:05:12.288184 139788571789120 model_lib_v2.py:708] {'Loss/classification_loss': 0.82894284,\n",
      " 'Loss/localization_loss': 0.6369645,\n",
      " 'Loss/regularization_loss': 0.0,\n",
      " 'Loss/total_loss': 1.4659073,\n",
      " 'learning_rate': 0.0293332}\n"
     ]
    }
   ],
   "source": [
    "# Execute training\n",
    "!python $current_dir/models/research/object_detection/model_main_tf2.py \\\n",
    "    --pipeline_config_path=$pipeline_config_path \\\n",
    "    --model_dir=$model_dir \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps=$NUM_STEPS \\\n",
    "    --run_once=False \\\n",
    "    --sample_1_of_n_eval_examples=1 \\\n",
    "    --eval_interval_secs=600 \\\n",
    "    --num_eval_steps=$NUM_EVAL_STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae63895-aa91-4ef8-9181-939af6da055d",
   "metadata": {},
   "source": [
    "The script above was used to train an object detection model using ```TensorFlow 2```. It takes in a pipeline configuration file, which specifies the model and training configuration, and a set of training and evaluation data.\n",
    "\n",
    "The script has several flags that can be used to control the training process. \n",
    "\n",
    "- The ```--pipeline_config_path``` flag specifies the path to the pipeline configuration file, which defines the model architecture and training parameters. \n",
    "\n",
    "- The ```--model_dir``` flag specifies the directory where the trained model and training logs should be saved. The --alsologtostderr flag causes the training logs to be written to both the log file and the console.\n",
    "\n",
    "- The ```--num_train_steps``` flag specifies the number of training steps to run.\n",
    "\n",
    "- The ```--num_eval_steps``` flag specifies the number of evaluation steps to run. \n",
    "\n",
    "- The ```--run_once``` flag tells the script to run evaluation once at the end of training if set to ```True```, or to run evaluation at regular intervals during training if set to ```False```. \n",
    "\n",
    "- The ```--sample_1_of_n_eval_examples``` flag specifies how many examples from the evaluation dataset should be used for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07bbbb-12b4-4e45-8ae4-d430c15dce9e",
   "metadata": {},
   "source": [
    "### Export our OD inference graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899df8e-8d20-4265-a9fb-3d244ad1af28",
   "metadata": {},
   "source": [
    "Graphs are data structures that contain a set of ```tf.Operation``` objects, which represent units of computation; and ```tf.Tensor``` objects, which represent the units of data that flow between operations. \n",
    "\n",
    "Here we will save our object detection inference graph files in ```ssd_inference_graph/saved_model```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10716f79-fc88-453f-9e57-e32b270eaff9",
   "metadata": {},
   "source": [
    "The following script uses the ```exporter_main_v2.py``` script from the TensorFlow object detection library to export the trained model. The script loads the trained model from the specified checkpoint directory and then uses the pipeline configuration file to create a new model (a copy of the trained model) with the same architecture. The exported model is saved in the specified output directory.\n",
    "\n",
    "This new model is a copy of the trained model, but it has been converted to a format that is suitable for serving or for further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "061b0e3a-065c-4ca8-902a-fa7a2e671128",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-22 13:32:16.408113: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-22 13:32:17.489770: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2022-12-22 13:32:17.489831: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2022-12-22 13:32:17.489840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-22 13:32:19.437146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2022-12-22 13:32:19.437174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: CMPR01\n",
      "2022-12-22 13:32:19.437182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: CMPR01\n",
      "2022-12-22 13:32:19.437421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.161.3\n",
      "2022-12-22 13:32:19.437442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\n",
      "2022-12-22 13:32:19.437449: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 460.91.3 does not match DSO version 470.161.3 -- cannot find working devices in this configuration\n",
      "2022-12-22 13:32:19.458282: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "W1222 13:32:19.519686 139664209377088 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "W1222 13:32:19.584636 139664209377088 deprecation.py:623] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1222 13:32:25.353399 139664209377088 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0\n",
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:459: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "W1222 13:32:30.621240 139664209377088 deprecation.py:350] From /home/msc1/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:459: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.faster_rcnn_meta_arch.FasterRCNNMetaArch object at 0x7f055cbb9cd0>, because it is not built.\n",
      "W1222 13:32:39.359387 139664209377088 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.meta_architectures.faster_rcnn_meta_arch.FasterRCNNMetaArch object at 0x7f055cbb9cd0>, because it is not built.\n",
      "W1222 13:32:56.879775 139664209377088 save.py:271] Found untraced functions such as FirstStageBoxPredictor_layer_call_fn, FirstStageBoxPredictor_layer_call_and_return_conditional_losses, mask_rcnn_keras_box_predictor_layer_call_fn, mask_rcnn_keras_box_predictor_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op while saving (showing 5 of 135). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: fasterrcnn_inference_graph/saved_model/assets\n",
      "I1222 13:33:03.000894 139664209377088 builder_impl.py:797] Assets written to: fasterrcnn_inference_graph/saved_model/assets\n",
      "INFO:tensorflow:Writing pipeline config file to fasterrcnn_inference_graph/pipeline.config\n",
      "I1222 13:33:04.188942 139664209377088 config_util.py:253] Writing pipeline config file to fasterrcnn_inference_graph/pipeline.config\n"
     ]
    }
   ],
   "source": [
    "# Define the output directory\n",
    "output_directory = 'ssd_inference_graph'\n",
    "\n",
    "# Export OD inference graph\n",
    "!python $current_dir/models/research/object_detection/exporter_main_v2.py \\\n",
    "    --trained_checkpoint_dir $model_dir \\\n",
    "    --output_directory $output_directory \\\n",
    "    --pipeline_config_path $pipeline_config_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d0820-e43e-4c36-bd04-8abe984dfa5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "- ``` trained_checkpoint_dir``` : Directory containing the trained model checkpoints.\n",
    "- ``` output_directory``` : Directory where the exported model will be saved.\n",
    "- ``` pipeline_config_path``` : Path to the pipeline configuration file, which specifies the model architecture and other options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f89a5-f50a-43ad-9496-ce255a61f27c",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Evaluate the trained model using TensorBoard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlowGPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
